#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å­¦ä¹ å™¨ V2 - ä»ç›®å½•æ‰¹é‡å­¦ä¹ æç¤ºè¯
æ”¯æŒå¤šç§æ ¼å¼ï¼Œè‡ªåŠ¨è´¨é‡æ£€æŸ¥ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime
from universal_learner_v2 import UniversalLearnerV2
from element_db import ElementDB
from txt_to_json_converter import TxtToJsonConverter


class PromptQualityChecker:
    """æç¤ºè¯è´¨é‡æ£€æŸ¥å™¨"""

    def __init__(self):
        self.min_length = 50  # æœ€å°é•¿åº¦
        self.max_length = 10000  # æœ€å¤§é•¿åº¦

    def check_quality(self, prompt_data: Dict) -> Tuple[bool, str, int]:
        """
        æ£€æŸ¥æç¤ºè¯è´¨é‡

        Returns:
            (is_valid, reason, quality_score)
        """
        original = prompt_data.get('original_prompt', '')

        # æ£€æŸ¥1: é•¿åº¦
        if len(original) < self.min_length:
            return False, f"å¤ªçŸ­ ({len(original)}å­—ç¬¦)", 0

        if len(original) > self.max_length:
            return False, f"å¤ªé•¿ ({len(original)}å­—ç¬¦)", 0

        # æ£€æŸ¥2: æ˜¯å¦æœ‰modules
        modules = prompt_data.get('modules', {})
        if not modules or len(modules) == 0:
            return False, "ç¼ºå°‘modulesç»“æ„", 2

        # æ£€æŸ¥3: moduleså¤æ‚åº¦
        module_count = len(modules)
        if module_count < 3:
            return True, "ç»“æ„ç®€å•", 5  # å¯ä»¥å­¦ä¹ ï¼Œä½†è´¨é‡ä¸€èˆ¬

        # æ£€æŸ¥4: æ˜¯å¦æœ‰æ•°ç»„æ•°æ®
        has_arrays = False
        array_count = 0
        for key, value in modules.items():
            if isinstance(value, list) and len(value) > 0:
                has_arrays = True
                array_count += 1

        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = 5  # åŸºç¡€åˆ†
        quality_score += min(module_count, 10)  # modulesæ•°é‡ (æœ€å¤š+10åˆ†)
        quality_score += array_count * 2  # æ•°ç»„æ•°é‡ (æ¯ä¸ª+2åˆ†)

        if quality_score >= 15:
            return True, "é«˜è´¨é‡", min(quality_score, 10)
        elif quality_score >= 10:
            return True, "ä¸­ç­‰è´¨é‡", min(quality_score, 10)
        else:
            return True, "åŸºç¡€è´¨é‡", min(quality_score, 10)


class BatchLearner:
    """æ‰¹é‡å­¦ä¹ å™¨"""

    def __init__(self, db_path: str = "extracted_results/elements.db"):
        self.learner = UniversalLearnerV2(db_path)
        self.quality_checker = PromptQualityChecker()
        self.txt_converter = TxtToJsonConverter()

        self.stats = {
            'total_files': 0,
            'processed': 0,
            'skipped': 0,
            'failed': 0,
            'total_added': 0,
            'total_extracted': 0,
            'by_quality': {
                'é«˜è´¨é‡': 0,
                'ä¸­ç­‰è´¨é‡': 0,
                'åŸºç¡€è´¨é‡': 0,
                'ä½è´¨é‡': 0
            },
            'skip_reasons': {},
            'failed_files': []
        }

    def load_prompt_file(self, file_path: Path) -> Dict:
        """åŠ è½½æç¤ºè¯æ–‡ä»¶ï¼ˆæ”¯æŒJSONå’ŒTXTï¼‰"""
        try:
            # å°è¯•JSONæ ¼å¼
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        except json.JSONDecodeError:
            # TXTæ ¼å¼ - ä½¿ç”¨è½¬æ¢å™¨è§£æ
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    txt_content = f.read()
                    # ä½¿ç”¨è½¬æ¢å™¨è‡ªåŠ¨æå–modules
                    return self.txt_converter.convert_txt_to_prompt_data(
                        txt_content,
                        file_path.name
                    )
            except Exception as e:
                raise Exception(f"æ— æ³•è¯»å–æ–‡ä»¶: {e}")

    def learn_from_directory(self, directory: str,
                            quality_threshold: int = 0,
                            dry_run: bool = False,
                            max_files: int = None) -> Dict:
        """
        ä»ç›®å½•æ‰¹é‡å­¦ä¹ 

        Args:
            directory: æç¤ºè¯æ–‡ä»¶ç›®å½•
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆ0-10ï¼‰ï¼Œä½äºæ­¤åˆ†æ•°çš„è·³è¿‡
            dry_run: é¢„æ¼”æ¨¡å¼ï¼Œåªæ£€æŸ¥ä¸å­¦ä¹ 
            max_files: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰
        """
        dir_path = Path(directory)
        if not dir_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory}")

        # æŸ¥æ‰¾æ‰€æœ‰JSONå’ŒTXTæ–‡ä»¶
        json_files = list(dir_path.glob("*.json"))
        txt_files = list(dir_path.glob("*.txt"))
        all_files = json_files + txt_files

        if max_files:
            all_files = all_files[:max_files]

        self.stats['total_files'] = len(all_files)

        print(f"\n{'='*80}")
        print(f"ğŸ“š æ‰¹é‡å­¦ä¹ å™¨ V2")
        print(f"{'='*80}")
        print(f"ç›®å½•: {directory}")
        print(f"æ–‡ä»¶æ€»æ•°: {len(all_files)} (JSON: {len(json_files)}, TXT: {len(txt_files)})")
        print(f"è´¨é‡é˜ˆå€¼: {quality_threshold}/10")
        print(f"æ¨¡å¼: {'ğŸ” é¢„æ¼”æ¨¡å¼' if dry_run else 'âœ… å­¦ä¹ æ¨¡å¼'}")
        if max_files:
            print(f"é™åˆ¶: æœ€å¤šå¤„ç† {max_files} ä¸ªæ–‡ä»¶")
        print(f"{'='*80}\n")

        # è·å–åˆå§‹æ•°æ®åº“çŠ¶æ€
        initial_stats = self.learner.db.get_stats()
        initial_elements = initial_stats['total_elements']

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for idx, file_path in enumerate(all_files, 1):
            print(f"\n[{idx}/{len(all_files)}] å¤„ç†: {file_path.name}")

            try:
                # åŠ è½½æ–‡ä»¶
                prompt_data = self.load_prompt_file(file_path)

                # è´¨é‡æ£€æŸ¥
                is_valid, reason, quality_score = self.quality_checker.check_quality(prompt_data)

                if not is_valid:
                    print(f"  â­ï¸  è·³è¿‡: {reason}")
                    self.stats['skipped'] += 1
                    self.stats['skip_reasons'][reason] = self.stats['skip_reasons'].get(reason, 0) + 1
                    continue

                if quality_score < quality_threshold:
                    print(f"  â­ï¸  è·³è¿‡: è´¨é‡åˆ†æ•° {quality_score}/10 ä½äºé˜ˆå€¼ {quality_threshold}")
                    self.stats['skipped'] += 1
                    self.stats['skip_reasons']['è´¨é‡åˆ†æ•°ä¸è¶³'] = self.stats['skip_reasons'].get('è´¨é‡åˆ†æ•°ä¸è¶³', 0) + 1
                    continue

                print(f"  âœ“ è´¨é‡: {reason} ({quality_score}/10)")
                self.stats['by_quality'][reason] = self.stats['by_quality'].get(reason, 0) + 1

                if dry_run:
                    print(f"  ğŸ” é¢„æ¼”æ¨¡å¼ - è·³è¿‡å­¦ä¹ ")
                    self.stats['processed'] += 1
                    continue

                # å­¦ä¹ 
                result = self.learner.learn_from_prompt(prompt_data)

                self.stats['processed'] += 1
                self.stats['total_added'] += result['added']
                self.stats['total_extracted'] += result['added'] + result['skipped']

                print(f"  âœ… å­¦ä¹ å®Œæˆ: æå– {result['added'] + result['skipped']} ä¸ªå…ƒç´ , æ·»åŠ  {result['added']} ä¸ª")

            except Exception as e:
                print(f"  âŒ å¤±è´¥: {str(e)}")
                self.stats['failed'] += 1
                self.stats['failed_files'].append({
                    'file': file_path.name,
                    'error': str(e)
                })

        # è·å–æœ€ç»ˆæ•°æ®åº“çŠ¶æ€
        final_stats = self.learner.db.get_stats()
        final_elements = final_stats['total_elements']

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(initial_elements, final_elements, dry_run)

        return report

    def _generate_report(self, initial_elements: int, final_elements: int, dry_run: bool) -> Dict:
        """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ‰¹é‡å­¦ä¹ æŠ¥å‘Š")
        print(f"{'='*80}\n")

        print(f"æ–‡ä»¶å¤„ç†ï¼š")
        print(f"  æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"  æˆåŠŸå¤„ç†: {self.stats['processed']}")
        print(f"  è·³è¿‡: {self.stats['skipped']}")
        print(f"  å¤±è´¥: {self.stats['failed']}")
        print()

        if self.stats['skip_reasons']:
            print(f"è·³è¿‡åŸå› ï¼š")
            for reason, count in sorted(self.stats['skip_reasons'].items(), key=lambda x: x[1], reverse=True):
                print(f"  â€¢ {reason}: {count} ä¸ª")
            print()

        print(f"è´¨é‡åˆ†å¸ƒï¼š")
        for quality, count in sorted(self.stats['by_quality'].items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"  â€¢ {quality}: {count} ä¸ª")
        print()

        if not dry_run:
            print(f"å­¦ä¹ æˆæœï¼š")
            print(f"  æå–å…ƒç´ æ€»æ•°: {self.stats['total_extracted']}")
            print(f"  æ–°å¢å…ƒç´ : {self.stats['total_added']}")
            print(f"  å»é‡è·³è¿‡: {self.stats['total_extracted'] - self.stats['total_added']}")
            print(f"  æ•°æ®åº“å¢é•¿: {initial_elements} â†’ {final_elements} (+{final_elements - initial_elements})")
            print()

        if self.stats['failed_files']:
            print(f"å¤±è´¥æ–‡ä»¶ ({len(self.stats['failed_files'])}ä¸ª)ï¼š")
            for item in self.stats['failed_files'][:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                print(f"  â€¢ {item['file']}: {item['error']}")
            if len(self.stats['failed_files']) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.stats['failed_files']) - 10} ä¸ª")
            print()

        print(f"{'='*80}\n")

        return {
            'stats': self.stats,
            'initial_elements': initial_elements,
            'final_elements': final_elements,
            'growth': final_elements - initial_elements
        }

    def close(self):
        """å…³é—­å­¦ä¹ å™¨"""
        self.learner.close()


# å‘½ä»¤è¡Œä½¿ç”¨
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æ‰¹é‡å­¦ä¹ æç¤ºè¯')
    parser.add_argument('directory', help='æç¤ºè¯æ–‡ä»¶ç›®å½•')
    parser.add_argument('--quality', type=int, default=0, help='è´¨é‡é˜ˆå€¼ (0-10), é»˜è®¤0')
    parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼ï¼Œåªæ£€æŸ¥ä¸å­¦ä¹ ')
    parser.add_argument('--max-files', type=int, help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--db', default='extracted_results/elements.db', help='æ•°æ®åº“è·¯å¾„')

    args = parser.parse_args()

    batch_learner = BatchLearner(args.db)

    try:
        report = batch_learner.learn_from_directory(
            directory=args.directory,
            quality_threshold=args.quality,
            dry_run=args.dry_run,
            max_files=args.max_files
        )

        # å¯¼å‡ºJSON
        if not args.dry_run and report['growth'] > 0:
            batch_learner.learner.db.export_to_json('extracted_results/universal_elements_library.json')
            print("âœ… å·²å¯¼å‡ºåˆ° universal_elements_library.json")

    finally:
        batch_learner.close()
